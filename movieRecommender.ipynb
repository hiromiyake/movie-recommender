{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation Project\n",
    "\n",
    "## May 15, 2017\n",
    "## Hiro Miyake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, I describe a movie recommender system using collaborative filtering implemented in Python. This project is inspired by the [Machine Learning](https://www.coursera.org/learn/machine-learning) Coursera course taught by Andrew Ng.\n",
    "\n",
    "**Table of Contents**\n",
    "1. Introduction\n",
    "2. Using the Program to Predict Movies You Might Like\n",
    "3. Theory of Collaborative Filtering\n",
    " 1. 5 Users and 5 Movies Example\n",
    " 2. Definitions\n",
    " 3. Cost Function\n",
    " 4. Optimization and Rating Predictions\n",
    "4. Optimizing Hyperparameters with Regularization\n",
    " 1. Optimizing the Regularization Parameter $\\lambda$\n",
    " 2. Optimizing the Number of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Companies such as [Netflix](https://www.netflix.com) and [Hulu](https://www.hulu.com) use sophisticated algorithms to recommend TV shows or movies that you might like. One approach one can take is to use the technique of [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering). Essentially, by knowing the ratings of movies from many different people, it is possible to predict what a new person may rate highly. I will go into further detail below on how this was implemented here.\n",
    "\n",
    "For this project, I use the [MovieLens](https://grouplens.org/datasets/movielens/) data, made available through the [GroupLens](https://grouplens.org) research lab at the University of Minnesota. In particular, I use their small data set generated on October 17, 2016, which contains 100004 ratings from 671 users and 9125 movies obtained between January 9, 1995 and October 16, 2016. The ratings take values between 1 and 5. For this project I use the files ```movies.csv``` and ```ratings.csv```. These can be downloaded from [here](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip). Further information can be found at the MovieLens website [here](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Program to Predict Movies You Might Like\n",
    "\n",
    "First let's start by demonstrating how you can use my code `movierec_v6.py` to generate movie recommendations. You need the files `movies.csv` and `ratings.csv` downloadable from [MovieLens](https://grouplens.org/datasets/movielens/latest/), and you need the Python file `movierec_v6.py`. This needs `numpy` and `scipy`.\n",
    "\n",
    "Once you have those, you need to create a Python dictionary of ```movieId``` and rating key-value pairs. You will have to directly look at the contents in the ```movies.csv``` file to figure out the ```movieId```. Below this dictionary is called ```my_ratings_dict```.\n",
    "\n",
    "Once you have the dictionary, you can feed this into the function ```movierec```, along with ```randseed``` which is the seed value for the random number generator in the function, and ```prednum``` which is the total number of movie ratings you want to predict. `lam` is the regularization parameter, and `num_features` is the total number of features we want to use for our optimization.\n",
    "\n",
    "Let's see how this works. My ratings are such that it should favor animated movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rated 1 for Texas Chainsaw Massacre, The (2003)\n",
      "Rated 5 for Incredibles, The (2004)\n",
      "Rated 5 for Frozen (2013)\n",
      "Rated 5 for Lion King, The (1994)\n",
      "Rated 5 for SpongeBob SquarePants Movie, The (2004)\n",
      "Rated 1 for Silence of the Lambs, The (1991)\n",
      "Rated 5 for Snow White and the Seven Dwarfs (1937)\n",
      "Rated 5 for Beauty and the Beast (1991)\n",
      "Rated 1 for Hannibal (2001)\n",
      "Rated 1 for Ouija (2014)\n",
      "Rated 1 for Saw (2004)\n",
      "Top 20 recommendations for you:\n",
      "1. Predicting rating 3.08 for movie Shawshank Redemption, The (1994)\n",
      "2. Predicting rating 3.06 for movie Schindler's List (1993)\n",
      "3. Predicting rating 2.89 for movie Babe (1995)\n",
      "4. Predicting rating 2.83 for movie Apollo 13 (1995)\n",
      "5. Predicting rating 2.82 for movie Wallace & Gromit: The Wrong Trousers (1993)\n",
      "6. Predicting rating 2.82 for movie Forrest Gump (1994)\n",
      "7. Predicting rating 2.80 for movie Wallace & Gromit: A Close Shave (1995)\n",
      "8. Predicting rating 2.78 for movie Aladdin (1992)\n",
      "9. Predicting rating 2.78 for movie Wallace & Gromit: The Best of Aardman Animation (1996)\n",
      "10. Predicting rating 2.78 for movie Finding Nemo (2003)\n",
      "11. Predicting rating 2.75 for movie Dances with Wolves (1990)\n",
      "12. Predicting rating 2.74 for movie Toy Story (1995)\n",
      "13. Predicting rating 2.73 for movie Star Wars: Episode IV - A New Hope (1977)\n",
      "14. Predicting rating 2.73 for movie Braveheart (1995)\n",
      "15. Predicting rating 2.72 for movie Fugitive, The (1993)\n",
      "16. Predicting rating 2.71 for movie Jurassic Park (1993)\n",
      "17. Predicting rating 2.70 for movie Monsters, Inc. (2001)\n",
      "18. Predicting rating 2.70 for movie Sense and Sensibility (1995)\n",
      "19. Predicting rating 2.67 for movie Shrek (2001)\n",
      "20. Predicting rating 2.67 for movie Godfather, The (1972)\n"
     ]
    }
   ],
   "source": [
    "import movierec_v6\n",
    "\n",
    "my_ratings_dict = {}\n",
    "\n",
    "## Like animation, hate horror\n",
    "my_ratings_dict[364] = 5    ## Lion King (1994)\n",
    "my_ratings_dict[594] = 5    ## Snow White (1937)\n",
    "my_ratings_dict[595] = 5    ## Beauty and the Beast (1991)\n",
    "my_ratings_dict[6880] = 1   ## Texas Chainsaw Massacre (2003)\n",
    "my_ratings_dict[8957] = 1   ## Saw (2004)\n",
    "my_ratings_dict[8974] = 5   ## SpongeBob SquarePants Movie, The (2004)\n",
    "my_ratings_dict[8961] = 5   ## Incredibles, The (2004)\n",
    "my_ratings_dict[106696] = 5 ## Frozen (2013)\n",
    "my_ratings_dict[115534] = 1 ## Ouija (2014)\n",
    "my_ratings_dict[4148] = 1   ## Hannibal (2001)\n",
    "my_ratings_dict[593] = 1    ## Silence of the Lambs, The (1991)\n",
    "\n",
    "movierec_v6.movierec(my_ratings_dict, randseed = 9999, prednum = 20, lam = 10, num_features = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the recommended list contains many movies consistent with children's movies.\n",
    "\n",
    "Now let's reverse those reviews and see how that changes the predicted movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rated 5 for Texas Chainsaw Massacre, The (2003)\n",
      "Rated 1 for Incredibles, The (2004)\n",
      "Rated 1 for Frozen (2013)\n",
      "Rated 1 for Lion King, The (1994)\n",
      "Rated 1 for SpongeBob SquarePants Movie, The (2004)\n",
      "Rated 5 for Silence of the Lambs, The (1991)\n",
      "Rated 1 for Snow White and the Seven Dwarfs (1937)\n",
      "Rated 1 for Beauty and the Beast (1991)\n",
      "Rated 5 for Hannibal (2001)\n",
      "Rated 5 for Ouija (2014)\n",
      "Rated 5 for Saw (2004)\n",
      "Top 20 recommendations for you:\n",
      "1. Predicting rating 2.53 for movie Lord of the Rings: The Two Towers, The (2002)\n",
      "2. Predicting rating 2.44 for movie Lord of the Rings: The Fellowship of the Ring, The (2001)\n",
      "3. Predicting rating 2.41 for movie Lord of the Rings: The Return of the King, The (2003)\n",
      "4. Predicting rating 2.39 for movie Braveheart (1995)\n",
      "5. Predicting rating 2.37 for movie Terminator 2: Judgment Day (1991)\n",
      "6. Predicting rating 2.35 for movie Shawshank Redemption, The (1994)\n",
      "7. Predicting rating 2.34 for movie Matrix, The (1999)\n",
      "8. Predicting rating 2.32 for movie Star Wars: Episode IV - A New Hope (1977)\n",
      "9. Predicting rating 2.32 for movie Usual Suspects, The (1995)\n",
      "10. Predicting rating 2.28 for movie Star Wars: Episode V - The Empire Strikes Back (1980)\n",
      "11. Predicting rating 2.27 for movie Dark Knight, The (2008)\n",
      "12. Predicting rating 2.25 for movie Seven (a.k.a. Se7en) (1995)\n",
      "13. Predicting rating 2.25 for movie Saving Private Ryan (1998)\n",
      "14. Predicting rating 2.25 for movie Fugitive, The (1993)\n",
      "15. Predicting rating 2.25 for movie Indiana Jones and the Last Crusade (1989)\n",
      "16. Predicting rating 2.24 for movie Jurassic Park (1993)\n",
      "17. Predicting rating 2.23 for movie Forrest Gump (1994)\n",
      "18. Predicting rating 2.23 for movie Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n",
      "19. Predicting rating 2.23 for movie Schindler's List (1993)\n",
      "20. Predicting rating 2.20 for movie Crimson Tide (1995)\n"
     ]
    }
   ],
   "source": [
    "## Like horror, hate animation\n",
    "my_ratings_dict[364] = 1    ## Lion King (1994)\n",
    "my_ratings_dict[594] = 1    ## Snow White (1937)\n",
    "my_ratings_dict[595] = 1    ## Beauty and the Beast (1991)\n",
    "my_ratings_dict[6880] = 5   ## Texas Chainsaw Massacre (2003)\n",
    "my_ratings_dict[8957] = 5   ## Saw (2004)\n",
    "my_ratings_dict[8974] = 1   ## SpongeBob SquarePants Movie, The (2004)\n",
    "my_ratings_dict[8961] = 1   ## Incredibles, The (2004)\n",
    "my_ratings_dict[106696] = 1 ## Frozen (2013)\n",
    "my_ratings_dict[115534] = 5 ## Ouija (2014)\n",
    "my_ratings_dict[4148] = 5   ## Hannibal (2001)\n",
    "my_ratings_dict[593] = 5    ## Silence of the Lambs, The (1991)\n",
    "\n",
    "movierec_v6.movierec(my_ratings_dict, randseed = 9999, prednum = 20, lam = 10, num_features = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is not very heavy on horror as one might expect from the provided ratings, but certainly includes more mature movies than the previous list.\n",
    "\n",
    "One thing to note is that some movies (such as Shawshank Redemption and Jurassic Park) appear in both lists, even though the user-provided ratings are polar opposites. This could be because these movies have many reviews and are highly rated, and so will tend to be highly recommended regardless of the initial ratings provided by the user.\n",
    "\n",
    "One approach to improving the prediction is for the user to provide more ratings. Also, providing ratings for movies with more reviews will probably lead to more accurate recommendations. More technically, it may make sense to penalize more heavily movies with lower ratings. Also, tuning the regularization parameter ```lam``` and number of features used for the optimization ```num_features``` could also be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Theory of Collaborative Filtering\n",
    "\n",
    "Now let's understand in more detail how the collaborative filtering algorithm actually works.\n",
    "\n",
    "### A. 5 Users and 5 Movies Example\n",
    "\n",
    "Let's consider an example with 5 movies reviewed by 5 people. In tabular format this can be shown as below.\n",
    "\n",
    "|Movie  |User 1|User 2|User 3|User 4|User 5|\n",
    "|:-----:|:----:|:----:|:----:|:----:|:----:|\n",
    "|Movie 1|5     |4     |2     |1     |4     |\n",
    "|Movie 2|4     |5     |2     |?     |?     |\n",
    "|Movie 3|1     |2     |4     |?     |?     |\n",
    "|Movie 4|?     |1     |?     |5     |2     |\n",
    "|Movie 5|4     |?     |5     |4     |?     |\n",
    "\n",
    "Note that there are many ? marks. These indicate that this user did not provide a review for this movie. Based on this incomplete table, we want to be able to fill in the question marks.\n",
    "\n",
    "To do this, we need to define a few parameters. First we have to decide how many parameters we need. One simple approach would be to consider one parameter for each movie, and one parameter for each user, but that won't provide us enough flexibility for prediction. On the other hand, if we have too many parameters, that would give us too much freedom for prediction. So a moderate number of parameters would be appropriate.\n",
    "\n",
    "For the sake of discussion, let us choose to use 3 parameters for each movie and 3 parameters for each user. Then take user 1 for example. User 1 can be characterized by a row vector $\\theta^{(1)}$ with 3 elements. Similarly movie 1 can be characterized by a different row vector $x^{(1)}$ with 3 elements. If we wanted to predict what user 1 would rate movie 1, we can take the dot product of the two vectors to get the predicted rating $p_{1,1} = \\sum_{i = 1}^3 x^{(1)}_i \\theta^{(1)}_i$. Likewise if we want to predict the rating of movie 3 for user 5, that is given by $p_{3,5} = \\sum_{i = 1}^3 x^{(3)}_i \\theta^{(5)}_i$.\n",
    "\n",
    "With these definitions, there is a natural definition of a cost function which should be minimized to get the best predicted ratings. Essentially we want to make $p_{i,j}$ as close to $y_{i,j}$ as possible for all pairs of $(i,j)$ where a rating was provided. Such a cost function can be mathematically written as\n",
    "\\begin{equation*}\n",
    "J_0 = \\sum_{i=1,j=1,y_{i,j} \\neq 0}^{i=5,j=5} (p_{i,j} - y_{i,j})^2\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "p_{i,j} = \\sum_{k = 1}^3 x^{(i)}_k \\theta^{(j)}_k.\n",
    "\\end{equation*}\n",
    "\n",
    "The cost function we just defined would serve its purpose, but it would be even better to include a regularization term to prevent overfitting of our data. This can be done with terms of the form\n",
    "\\begin{equation*}\n",
    "J_{\\rm reg} = \\frac{\\lambda}{2} \\sum_{i=1,j=1}^{i=3,j=5} \\left[ \\left( \\theta_i^{(j)} \\right)^2 + \\left( x_i^{(j)} \\right)^2 \\right].\n",
    "\\end{equation*}\n",
    "\n",
    "Note that this form of regularization is reminiscent of [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization). The total cost function then is $J = J_0 + J_{\\rm reg}$. Note that $J$ is a function of $x$ and $\\theta$. So we can minimize this cost function and get the optimum values for $x$ and $\\theta$, and then based on those values, predict movie ratings.\n",
    "\n",
    "### B. Definitions\n",
    "\n",
    "Now we define some relevant parameters.\n",
    "\n",
    "* $u$ is the number of users.\n",
    "* $m$ is the number of movies.\n",
    "* $f$ is the number of features we decide to use.\n",
    "* $R$ is a $m \\times u$ matrix with $R_{i,j} = 1$ if user $j$ rated movie $i$, and $R_{i,j} = 0$ otherwise.\n",
    "* $Y$ is a $m \\times u$ matrix with $Y_{i,j}$ equal to the rating (between 1 and 5) of movie $i$ provided by user $j$, and $Y_{i,j} = 0$ otherwise.\n",
    "* $\\Theta$ is a $u \\times f$ parameter matrix, whose values are randomly initialized to seed the optimization algorithm. This is a matrix of feature values for each user.\n",
    " * $\\theta^{(j)}$ is a $1 \\times f$ parameter row vector for user $j$. In other words, $\\Theta_{j,i} = \\theta^{(j)}_i$.\n",
    "* $X$ is a $m \\times f$ feature matrix, whose values are randomly initialized to seed the optimization algorithm. This is a matrix of feature values for each movie.\n",
    " * $x^{(i)}$ is a $1 \\times f$ feature row vector for movie $i$. In other words, $X_{i,j} = x^{(i)}_j$.\n",
    "* $\\lambda$ is a scalar regularization parameter.\n",
    "\n",
    "The prediction algorithm tries to predict the ratings for movies which have not been reviewed yet. In other words, the algorithm predicts a value $Y_{i,j}$ for entries where $R_{i,j}=0$.\n",
    "\n",
    "### C. Cost Function\n",
    "\n",
    "The cost function without regularization can be written\n",
    "\\begin{equation*}\n",
    "J_0(X,\\Theta) = \\frac{1}{2} \\sum_{(i,j):R(i,j)=1} \\left( (\\theta^{(j)})^T x^{(i)} - y_{i,j} \\right)^2,\n",
    "\\end{equation*}\n",
    "where the sum is over all $i$ and $j$ only for those pairs where $R(i,j) = 1$. Note I use $R_{i,j}$ and $R(i,j)$ interchangeably to denote the ($i$,$j$) element of the matrix $R$ if there is little cause for confusion.\n",
    "\n",
    "The regularization term of the cost function can be written\n",
    "\\begin{equation*}\n",
    "J_{\\rm reg}(X,\\Theta) = \\frac{\\lambda}{2} \\sum_{j=1}^u \\sum_{k=1}^f \\left( \\theta_k^{(j)} \\right)^2 + \\frac{\\lambda}{2} \\sum_{i=1}^m \\sum_{k=1}^f \\left( x_k^{(i)} \\right)^2.\n",
    "\\end{equation*}\n",
    "\n",
    "The total regularized cost function is then give as\n",
    "\\begin{equation*}\n",
    "J = J_0 + J_{\\rm reg}.\n",
    "\\end{equation*}\n",
    "\n",
    "The gradient of the regulaized cost function is given as\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial x_k^{(i)}} &= \\sum_{j:R(i,j)=1} \\left( (\\theta^{(j)})^T x^{(i)} - y_{i,j} \\right) \\theta_k^{(j)} + \\lambda x_k^{(i)} \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_k^{(j)}} &= \\sum_{i:R(i,j)=1} \\left( (\\theta^{(j)})^T x^{(i)} - y_{i,j} \\right) x_k^{(i)} + \\lambda \\theta_k^{(j)}.\n",
    "\\end{align}\n",
    "\n",
    "In the actual code, I seed $X$ and $\\Theta$ with random numbers to begin the optimization.\n",
    "\n",
    "### D. Optimization and Rating Predictions\n",
    "\n",
    "These regularized cost function gradients can be fed into an optimization algorithm to find the optimum $X$ and $\\Theta$ that minimize $J$. Here I use the ```minimize``` function in the ```scipy.optimize``` package. Once the optimum values are determined, we can find the predicted rating matrix $P$ through\n",
    "\\begin{equation*}\n",
    "P = X \\Theta^T.\n",
    "\\end{equation*}\n",
    "If the user ratings were in the first column, then the predicted user ratings are given by the first column, so for example the predicted rating for movie $i$ is given by $p_{\\rm user}(i) = P(i,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizing Hyperparameters with Cross-Validation\n",
    "\n",
    "One of the best ways to systematically determine the optimum value of hyperparameters such as the regularization parameter $\\lambda$ and the number of features is to use the technique of cross-validation (CV). In particular, $k$-fold cross-validation is a good option, where $k=5$. This is done by dividing the given users randomly into 5 groups, train the model on 4 of the groups, and try to predict on the fifth, and then determine the cost function on that prediction group. We then average the 5 possible ways to divide the 5 groups into 4 and 1, and look at the average cost function as a function of $\\lambda$ and the number of features.\n",
    "\n",
    "For CV, we have to do something a little different from what we did before. This is because with the approach we outlined above, each new user must be added to train the model from scratch. There is no training and test set in the traditional sense. Lets say we split the reviews into 5 groups, with 4 being the training set and 1 being the test set. We can run our algorithm above to train on the 4 training sets. Then we do something different for the test set. For each user $u_{\\rm i}'$ in the test set, we try to find a decomposition of the user in terms of users $u_j$ from the 4 training sets. So $u_{\\rm i}' = \\sum_j \\alpha_{ij} u_j$. Then we can make a prediction of user $i$ based on the prediction matrices $X$ and $\\Theta$ obtained from the training set. This means that for user $i$, the predicted rating for movie $j$ is $\\sum_k \\alpha_{ik} p_{j,k} = A P^T$ where $p_{j,k} = \\sum_m x_m^{(j)} \\theta_m^{(k)} = X \\Theta^T$, where $p_{\\rm j = movie, k = user}$. This means $A \\Theta X^T$. And then we can find the cost function for that user for each movie that they reviewed. Add those up for each user in the test set, and you have the total cost function we can use for CV. Note that for this to work, the users in the test set must have some overlap with the users in the training set. For example, if the training set includes only users who have rated action movies and the test set includes only users who have rated romance movies, this method will not work. However, given 671 users, splitting this into 5 parts randomly should avoid such a pathological situation.\n",
    "\n",
    "### A. Optimizing the Regularization Parameter $\\lambda$\n",
    "\n",
    "### B. Optimizing the Number of Features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
